<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AInsight Principles</title>
    <link rel="stylesheet" href="styles/styles.css">

</head>
<body>
    <header>
        <a id="logo-link" href="index.html">
            <img class="logo" src="images/AInsight-logos.jpeg" alt="Broken Canoe Logo Image">
        </a>
     
        <nav>
            <a href="index.html">Home</a>
            <a href="page01.html">Page 1</a>
            <a href="page02.html">Page 2</a>
            <a href="contactus.html">Contact Us</a>
        </nav>
    </header>

    <img class="banner-img1" src="images/math-circles-copy.jpg" alt="Cool design">

    <h1 style="text-align: center;">Principles of Machine Learning</h1>
    <p style="text-align: center;">There are very important principles that are used in machine learning. Principles of how work, gini, loss, gradient decent, some of the important ideas that allow models to learn are entropy, loss, and gradients. These ideas are at play in different models and some of them overlap. </p>

    <div id="info-block">
        <h4>Entropy</h4>
        <img src="images/entropy.png" alt="entropy diagram">
        <p>Entropy measures uncertainty or disorder. High entropy indicates high disorder. We can see that entropy is maximized at p=0.5. A value of p=0.5 means we are 50 certain that a sample belongs to class, thus the uncertainty is great. However, at p=0.9, we are 90 certain, so entropy is low. At p=0.1 , although we are 10 only sure that the sample belongs to one class, but that also means we 90 are sure it belongs to the other, thus our certainty is still high.</p>
    
 
        <h4>Loss</h4>
        <img src="images/loss.webp" alt="entropy diagram">
        <p>Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. </p>
    
        <h4>Gradient</h4>
        <img  src="images/SGD.jpeg" alt="entropy diagram">
        <p> A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. In mathematical terms, a gradient is a partial derivative with respect to its inputs. </p>
    </div>





    <footer>
        <p>AInsight &copy; 2022 - Erik Sanders</p>
        <p><a href="site-plan.html">Site Plan</a></p>
        <p><a href="contactus.html">Contact Us</a></p>
        <div class="social">
            <a href="https://facebook.com">
                <img src="images/facebook.png" alt="fb icon">
            </a>
            <a href="https://twitter.com">
                <img src="images/twitter.png" alt="twitter icon">
            </a>
            <a href="https://instagram.com">
                <img src="images/instagram.png" alt="instagram icon">
            </a>
        </div>
    </footer>

    
</body>
</html>